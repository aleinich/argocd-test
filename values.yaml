common_storage: &storage_size "5Gi"

common_probes: &probe_config
  livenessProbe:
    initialDelaySeconds: 30
    periodSeconds: 15
  readinessProbe:
    initialDelaySeconds: 30
    periodSeconds: 15

node1:
  fullnameOverride: "vm-node1"
  server:
    annotations: 
      argocd.argoproj.io/sync-wave: "1"
    persistentVolume: { enabled: true, size: *storage_size }
    <<: *probe_config
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: app.kubernetes.io/instance
              operator: In
              values: ["vm-node1", "vm-node2"]
          topologyKey: "kubernetes.io/hostname"
  vmagent: { enabled: false }

node2:
  fullnameOverride: "vm-node2"
  server:
    annotations: 
      argocd.argoproj.io/sync-wave: "2"
    persistentVolume: { enabled: true, size: *storage_size }
    <<: *probe_config
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: app.kubernetes.io/instance
              operator: In
              values: ["vm-node1", "vm-node2"]
          topologyKey: "kubernetes.io/hostname"
  vmagent: { enabled: false }
  
vmauth:
  enabled: true
  fullnameOverride: "vm-gateway"
  
  server:
    # Ora che non abbiamo vincoli, 2 repliche gireranno senza problemi
    replicaCount: 2
    
    # Questa struttura forza finalmente il RollingUpdate
    strategy:
      type: RollingUpdate
      rollingUpdate:
        maxUnavailable: 1
        maxSurge: 1
    
    # Fondamentali per far capire a K8s quando il pod Ã¨ pronto
    livenessProbe:
      initialDelaySeconds: 15
      tcpSocket:
        port: http
    readinessProbe:
      initialDelaySeconds: 15
      tcpSocket:
        port: http

  # Abbiamo rimosso completamente il blocco affinity

  config:
    unauthorized_user:
      url_prefix: 
        - "http://vm-node1-server:8428"
        - "http://vm-node2-server:8428"
